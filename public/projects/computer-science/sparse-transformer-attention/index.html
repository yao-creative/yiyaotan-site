<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Sparse Transformer Attention and Visual Image Transformer Segmentation | Yi Yao Tan</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="My most significant project, through a five-month research internship during my undergraduate at Huawei, I created routines for sparse attention in Transformers and tested their efficiencies on NLP data sets BERT model and image classification and segmentation on ImageNet-1k (100GB) with the Visual Image Transformer Segmenter. Created data pipelines, and evaluation metrics, worked on model design, started the transformer image segmentation library for Huawei, and trained models on Huawei’s Atlas 800 HPC clusters using the newly developed, parallelization-first ML framework MindSpore.">
    <meta name="generator" content="Hugo 0.129.0">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Yi Yao Tan">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://yiyaotan-site.vercel.app/projects/computer-science/sparse-transformer-attention/">
    

    <meta property="og:url" content="https://yiyaotan-site.vercel.app/projects/computer-science/sparse-transformer-attention/">
  <meta property="og:site_name" content="Yi Yao Tan">
  <meta property="og:title" content="Sparse Transformer Attention and Visual Image Transformer Segmentation">
  <meta property="og:description" content="My most significant project, through a five-month research internship during my undergraduate at Huawei, I created routines for sparse attention in Transformers and tested their efficiencies on NLP data sets BERT model and image classification and segmentation on ImageNet-1k (100GB) with the Visual Image Transformer Segmenter. Created data pipelines, and evaluation metrics, worked on model design, started the transformer image segmentation library for Huawei, and trained models on Huawei’s Atlas 800 HPC clusters using the newly developed, parallelization-first ML framework MindSpore.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2023-03-01T00:00:00+01:00">
    <meta property="article:modified_time" content="2023-03-01T00:00:00+01:00">

  <meta itemprop="name" content="Sparse Transformer Attention and Visual Image Transformer Segmentation">
  <meta itemprop="description" content="My most significant project, through a five-month research internship during my undergraduate at Huawei, I created routines for sparse attention in Transformers and tested their efficiencies on NLP data sets BERT model and image classification and segmentation on ImageNet-1k (100GB) with the Visual Image Transformer Segmenter. Created data pipelines, and evaluation metrics, worked on model design, started the transformer image segmentation library for Huawei, and trained models on Huawei’s Atlas 800 HPC clusters using the newly developed, parallelization-first ML framework MindSpore.">
  <meta itemprop="datePublished" content="2023-03-01T00:00:00+01:00">
  <meta itemprop="dateModified" content="2023-03-01T00:00:00+01:00">
  <meta itemprop="wordCount" content="82">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Sparse Transformer Attention and Visual Image Transformer Segmentation">
  <meta name="twitter:description" content="My most significant project, through a five-month research internship during my undergraduate at Huawei, I created routines for sparse attention in Transformers and tested their efficiencies on NLP data sets BERT model and image classification and segmentation on ImageNet-1k (100GB) with the Visual Image Transformer Segmenter. Created data pipelines, and evaluation metrics, worked on model design, started the transformer image segmentation library for Huawei, and trained models on Huawei’s Atlas 800 HPC clusters using the newly developed, parallelization-first ML framework MindSpore.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Yi Yao Tan
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/blog/" title="Blog page">
              Blog
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/resume/" title="Resume page">
              Resume
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Sparse Transformer Attention and Visual Image Transformer Segmentation
        </h1>
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Computer Science Projects
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Sparse Transformer Attention and Visual Image Transformer Segmentation</h1>
      
      <p class="tracked">
        By <strong>Yi Yao Tan</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2023-03-01T00:00:00+01:00">March 1, 2023</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>My most significant project, through a five-month research internship during my undergraduate at Huawei, I created routines for sparse attention in Transformers and tested their efficiencies on NLP data sets BERT model and image classification and segmentation on ImageNet-1k (100GB) with the Visual Image Transformer Segmenter. Created data pipelines, and evaluation metrics, worked on model design, started the transformer image segmentation library for Huawei, and trained models on Huawei’s Atlas 800 HPC clusters using the newly developed, parallelization-first ML framework MindSpore.</p>
<ul>
<li><a href="https://drive.google.com/file/d/17GK_U6A_BJJWpe7aI4YoQmDiVWY2DfW9/view?usp=sharing">Manuscript</a></li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://yiyaotan-site.vercel.app/" >
    &copy;  Yi Yao Tan 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
